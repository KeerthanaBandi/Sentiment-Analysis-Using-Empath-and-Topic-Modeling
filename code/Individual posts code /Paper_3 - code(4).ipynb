{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a6f57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9dc6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up the Reddit API\n",
    "user_agent = 'MyApp/1.0 by Nashville shootings'\n",
    "reddit = praw.Reddit(\n",
    "    client_id = '4xIYX_X2iMiwGhP4aZtE2g',\n",
    "    client_secret = 'xLymU0PVKlmS3cofzsEKG1fvsiWyrw',\n",
    "    user_agent = user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9166be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it so wrong to theorize that the Nashville gunman probably shot up the Christian school because he was bullied for being transgender?\n",
    "\n",
    "submission = reddit.submission(id='124otcv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a7ae7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def process_comments(comments, writer):\n",
    "    for comment in comments:\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            process_comments(comment.comments(), writer)\n",
    "        else:\n",
    "            writer.writerow({\n",
    "                'title': '',\n",
    "                'author': comment.author.name if comment.author else '[deleted]',\n",
    "                'score': comment.score,\n",
    "                'date': datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'comment': comment.body\n",
    "            })\n",
    "            process_comments(comment.replies, writer)\n",
    "\n",
    "with open('reddit_data_124otcv.csv', mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "    fieldnames = ['title', 'author', 'score', 'date', 'comment']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    writer.writerow({\n",
    "        'title': submission.title,\n",
    "        'author': submission.author.name,\n",
    "        'score': submission.score,\n",
    "        'date': datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'comment': ''\n",
    "    })\n",
    "\n",
    "    process_comments(submission.comments, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48919098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text) :  # Check if text is NaN\n",
    "        return ''\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs using regular expressions\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]+\", \" \", text)\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "# Define the path to your input CSV file\n",
    "input_file = \"/Users/keerthanabandi/Desktop/Semester - 2/Social Media Mining/paper 3/data/reddit_data_124otcv.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['comment'] = df['comment'].apply(preprocess_text)\n",
    "\n",
    "# Define the path for the output CSV file\n",
    "output_file = \"/Users/keerthanabandi/Desktop/Semester - 2/Social Media Mining/paper 3/data/124otcv.csv\"\n",
    "\n",
    "# Save the preprocessed DataFrame to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Saved preprocessed data to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your input CSV file\n",
    "input_file = \"/Users/keerthanabandi/Desktop/Semester - 2/Social Media Mining/paper 3/data/reddit_data_124otcv.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Remove rows where the 'comment' column is equal to 'removed'\n",
    "df = df[df['comment'] != 'removed' ]\n",
    "df = df[df['comment'] != 'deleted' ]\n",
    "# Define the path for the output CSV file\n",
    "output_file = \"/Users/keerthanabandi/Desktop/Semester - 2/Social Media Mining/paper 3/data/file4.csv\"\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Completed. Filtered data has been saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa495b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import empath\n",
    "\n",
    "# Open the input CSV file\n",
    "with open('file3.csv', mode='r', encoding='utf-8') as input_file:\n",
    "\n",
    "    # Create a CSV reader object for the input file\n",
    "    reader = csv.DictReader(input_file)\n",
    "\n",
    "    # Open the output CSV file\n",
    "    with open('sentiment_file4.csv', mode='w', encoding='utf-8', newline='') as output_file:\n",
    "\n",
    "        # Define the fieldnames for the output CSV file\n",
    "        fieldnames = reader.fieldnames + ['positive_score', 'negative_score', 'neutral_score', 'emotion']\n",
    "\n",
    "        # Create a CSV writer object for the output file\n",
    "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header row to the output file\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Create an instance of the Empath class\n",
    "        empath_analyzer = empath.Empath()\n",
    "\n",
    "        # Iterate through each row in the input file\n",
    "        for row in reader:\n",
    "\n",
    "            # Analyze the sentiment of the comment using Empath\n",
    "            sentiment = empath_analyzer.analyze(row['comment'], categories=['positive_emotion', 'negative_emotion', 'neutral_emotion'])\n",
    "\n",
    "            # Add the sentiment scores to the row dictionary\n",
    "            row['positive_score'] = sentiment['positive_emotion']\n",
    "            row['negative_score'] = sentiment['negative_emotion']\n",
    "            row['neutral_score'] = sentiment['neutral_emotion']\n",
    "\n",
    "            # Determine the most relevant emotion for the comment\n",
    "            if sentiment['positive_emotion'] > sentiment['negative_emotion'] and sentiment['positive_emotion'] > sentiment['neutral_emotion']:\n",
    "                row['emotion'] = 'positive'\n",
    "            elif sentiment['negative_emotion'] > sentiment['positive_emotion'] and sentiment['negative_emotion'] > sentiment['neutral_emotion']:\n",
    "                row['emotion'] = 'negative'\n",
    "            else:\n",
    "                row['emotion'] = 'neutral'\n",
    "\n",
    "            # Write the row to the output file\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv('sentiment_file4.csv')\n",
    "\n",
    "# group by emotion and sum the scores\n",
    "emotion_counts = df.groupby('emotion').sum()\n",
    "\n",
    "# create a pie chart\n",
    "plt.pie(emotion_counts['score'], labels=emotion_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "# set the title\n",
    "plt.title('Emotion Percentage')\n",
    "\n",
    "# display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import empath\n",
    "\n",
    "# create an instance of the empath library\n",
    "empath_analyzer = empath.Empath()\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv('file4.csv')\n",
    "\n",
    "# define a list of relevant emotions\n",
    "relevant_emotions = ['anger', 'sadness', 'depression', 'faith', 'hope', 'lost', 'hate', 'love','help','torment','pride']\n",
    "\n",
    "# add columns for each relevant emotion\n",
    "for emotion in relevant_emotions:\n",
    "    df[emotion] = 0\n",
    "\n",
    "# loop through each comment and analyze the emotions\n",
    "for i, row in df.iterrows():\n",
    "    comment = row['comment']\n",
    "    if isinstance(comment, str):\n",
    "        emotions = empath_analyzer.analyze(comment, categories=relevant_emotions)\n",
    "        for emotion in relevant_emotions:\n",
    "            df.loc[i, emotion] = emotions[emotion]\n",
    "\n",
    "# add a column for the dominant emotion\n",
    "df['dominant_emotion'] = df[relevant_emotions].idxmax(axis=1)\n",
    "\n",
    "# save the updated DataFrame to a new CSV file\n",
    "df.to_csv('nashville_shooting_sentiment_analysis3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the CSV file\n",
    "df = pd.read_csv('nashville_shooting_sentiment_analysis3.csv')\n",
    "\n",
    "# group by emotion and sum the scores\n",
    "emotion_counts = df[df['score'] >= 0].groupby('dominant_emotion').sum()\n",
    "\n",
    "# create a pie chart\n",
    "plt.pie(emotion_counts['score'], labels=emotion_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "# set the title\n",
    "plt.title('Emotion Percentage')\n",
    "\n",
    "# display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# get the post by its ID\n",
    "post = reddit.submission(id='126mjlb')\n",
    "\n",
    "# create a list to hold the comments\n",
    "comments = []\n",
    "\n",
    "# iterate over the comments and add them to the list\n",
    "for comment in post.comments.list():\n",
    "    # check if the comment has sub-comments\n",
    "    if isinstance(comment, praw.models.MoreComments):\n",
    "        continue\n",
    "    else:\n",
    "        comments.append(comment.body)\n",
    "\n",
    "# define the list of stop words\n",
    "stop_words = ['a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'what', 'which', 'this', 'that', 'these', 'those', 'then', 'just', 'so', 'than', 'such', 'both', 'through', 'about', 'for', 'with', 'without', 'within', 'between', 'into', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','is','on','of']\n",
    "\n",
    "# preprocess the comments by removing stop words and tokenizing the text\n",
    "processed_comments = []\n",
    "for comment in comments:\n",
    "    processed_comment = simple_preprocess(comment)\n",
    "    processed_comment = [word for word in processed_comment if word not in stop_words]\n",
    "    processed_comments.append(processed_comment)\n",
    "\n",
    "# create a dictionary from the comments\n",
    "dictionary = Dictionary(processed_comments)\n",
    "\n",
    "# create a bag-of-words representation of the comments\n",
    "bow_corpus = [dictionary.doc2bow(comment) for comment in processed_comments]\n",
    "\n",
    "# define the number of topics to find\n",
    "num_topics = 10\n",
    "\n",
    "# train the LDA model on the comments\n",
    "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# print the top 10 words for each topic\n",
    "for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f'Topic: {idx}')\n",
    "    print(f'Words: {topic}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
